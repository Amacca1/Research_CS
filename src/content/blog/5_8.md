---
title: 'Sixth Day - 5/8'
description: 'Tree of Thoughts and Chain of Preference Optimisation'
pubDate: 'Aug 5 2025'
heroImage: '../../assets/blog-placeholder-3.jpg'
---

After yesterday looking into Chain of Thought prompting and reasoning, I found more developments and changes that allow it to work better and more efficiently with more general and vague tasks, that require exploration and strategy to approach them correctly. The Tree of Thoughts is a good way to change the framework to allow these more general/complex tasks to be completed.

#### What is the Tree Of Thoughts?

Instead of following a single linear chain of reasoning, Tree of Thoughts allows the model to explore multiple reasoning paths simultaneously, much like how humans consider various approaches when solving difficult problems. The "tree" structure represents different branches of thought that can be explored, evaluated, and either pursued further or abandoned. This overcomes the normal 'left to right/token level' pattern that models originally used to take to make decisions during inference. 

These multiple reasoning paths (called 'thoughts') are the LLMs way to mimick human reasoning patterns (multiple ways to go about solving complex problems), while also allowing the ability to look ahead at what each thought produces, and also the ability to backtrack if a thought ends up being a poor choice.

This shift to allow for multiple strands of thinking also is solidifying how we mimick human cognition with LLMs, to allow them to engage in more complex tasks using a different style of thinking compared to the system 1 style token based, left to right style thinking. The Tree of Thoughts further allows complex thinking, known as system 2 thinking in human cognition research.

![](/assets/tot.png)

Despite it being a step onwards from the initial working Language models, and an improvement from the 2022 changes to allow more complex reasoning tasks, its origin and idea was first considered in the 1950s.
#### Improving Chain Of Thought Reasoning in LLMs