---
title: 'Looking at how the prompt and tool access affects quality'
description: 'Seeing if a prompt and its quality can affect the reliability and quality of the response.'
pubDate: 'Aug 12 2025'
---

After looking at the 6 Key Principles For Production AI Agents, I was interested in a certain question which I had to answer myself:

> How useful are prompts and tools really? 

This sounds like a silly question. Of course they are important, it has allowed a number of innovations and the general increase of quality prompts (especially consider the context revolution we are seeing).

So I propose looking into the response by a given prompt by 3 factors:

1. The quality of the response
2. The reliability/ reproducibility of a response
3. How many tokens did it use? (ie how expensive was the response)

In order to make this a fair test, I will need to consider a number of factors to ensure fair testing;
- The Large Language Model used - this idea is key to the principle of "Investing in your system prompt" as Arseni Kravchenko mentioned.
- The length of the initial prompt
- The tools available
- The type of response it needs to answer with
- The type and amount of context going into the LLM.

(The type and amount of context goes hand in hand with the length of the prompt and the tools that are being able to be used.)

Let us first compare the quality of the responses given these factors.

##### Problem Solving Task

First we need to consider a basic task: Solving an simple maths equation. This is a good baseline for a repeatable, reliable high accuracy response.

##### Creative Writing Task

##### Coding Task

##### Ethical Reasoning Task

##### Information Gathering

##### Opinion seeking

