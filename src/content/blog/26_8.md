---
title: 'Continuous Calibration/Continuous Development framework - Where Do Evals Play a Part?'
description: ''
pubDate: 'Aug 28 2025'
---

I have been looking into the Continuous Calibration/ Continuous Development (CC/CD)framework these past couple of days in regards to my most current project, a document reader that reads 
bureaucratic/ government documents, and using a RAG pipeline provides answers to queries you have in the document by quoting directly from the text. It uses a FAISS index for the data that you load into the document (It is persistent on disk, and gets loaded into memory when the code is currently ran). It then calls Claude API to find and write the relevant section(s) of the document, and in future will show and use direct links to other government documents (for now we are only testing on a random chapter 2 from JTTP 4-05). The link to the repo is [here](https://github.com/Amacca1/DocReader/settings/access). 

#### Evals

As part of the CC/CD framework, it has 7 sections (6 key parts, 1 transition part):

1. Scope Capability and Curate Data
2. Set up Application
3. Design Evals
Transition - Deploy here
4. Run Evals
5. Analyse behaviour and spot error patterns
6. Apply fixes
Deploy and back to step 1. 

This blog post will be about the steps 3 and 4 - The evaluation metrics (evals). 

First, we need to consider if my current evals are currently being used in the way that the CC/CD framework wants them to be used. 

From the [wonderful blog](https://www.lennysnewsletter.com/p/why-your-ai-product-needs-a-different) (Lenny's Newsletter) about this framework, this is how they describe and differentiate these evals to normal tests:

> Evals are scoring mechanisms that help you assess whether your AI system is working, where it’s falling short, and what needs improvement. Think of them as the equivalent of tests in traditional software. But unlike typical unit or integration tests, where inputs and outputs are fixed and correctness is binary, AI evals deal with ambiguity.

So our evals need to be utilising the application that has been set up in a non deterministic fashion, So we need to consider a number of factors. 

##### What do I have currently?

What I currently have in the evals.py could be considered to be a *Reference Dataset*. This is a reference for what expected behaviour looks like and how the AI should respond in my system. So I should treat it in that way, similar to few shot context systems and how examples are used in Chain of Thought prompting. As I was making this previously, I tried to consider this to be the actual designed evals, so this is an incorrect style of thinking in this framework. 

I will now style these in accordance to using it as an encoded part of the data provided to the Claude API, to allow for better answering and being part of a feedback loop with the logs of queries to build an overall bank of good examples.

In regards to this dataset, I believe a good response should have these things:

- Source/Paragraph reference
- Nothing summarised (which can lose data)
- All relevant sections written (from most to possibly important).

For now I am unsure what to do with tables. 


##### What Factors should I consider in making a Eval?

I need to consider a number of factors while creating the evaluation metrics. There is also another [piece on writing evals](https://www.lennysnewsletter.com/p/beyond-vibe-checks-a-pms-complete) on Lenny's Newsletter. 
We need to consider:
- Hallucination: Does it make up data not present in the Chapter 2 markdown file?
- Correctness: Does it actually get the data the user needs from their query?
- Does it retrieve all relevant data throughout the Chapter 2 file?

So to evaluate our RAG program performance, we most likely need to use our reference dataset to test it (as highlighted in both the [huggingface RAG evaluation](https://huggingface.co/learn/cookbook/en/rag_evaluation) and the previous CC/CD framework blog linked before) and an evaluator to compute accuracy. We can actually use other LLMs to judge these factors from the responses we provide. I will use another anthropic model to check against the original response. (Sonnet to check, Haiku to write)

#### Results:

I tweaked how the evals.py file worked. I used a LLM-judge method to look at the response that the rag.py file created, then compared it to 3 Boolean items:

> cites_all_relevant: Does it site all relevant information?

> not_paraphrasing: Is it writing the exact section as it is written?

>cites_sections_paragraphs: Does it cite all information that it gives from the document?

It then collates and gives it a score from 0 (worst) to 1 (best). It then also gives notes on the response, then writes an improvement section to allow the user to provide better feedback. To get the queries, it gets a text file of your choosing and runs each query 1 by 1 to allow each query to have separate feedback overall. This will be useful for when I create and use logs to also tweak and improve the system. It is placed into a testing JSON file to allow for easier reading. 

Example test command line:

```
uv run python evals.py \
  --queries-file test_query.txt \
  --run-real \
  --judge-model "claude-3-5-haiku-latest" \
  --output test_results.json
```

I also added some few shot examples to the system, to overall steer the system into the style of response that I wanted it to mimic. Again this can be further improved as the system is scaled up to also read the logs of previous inputs to further improve the system. 

```Python
EXAMPLE_QA_PAIRS = [
    {
        "question": "What is the recommended occupancy for junior ranks?",
        "answer": "According to Table 2D.1 in section 2D3, the occupancy for junior ranks is 4 personnel."
    },
    {
        "question": "What are Key User Requirements (KURs)?", 
        "answer": "a. Priority. A priority should be assigned to each element of requirement.13 (1) Key User Requirements (KURs), are the Critical Success Factors without which the facility/service will not possibly be fit for purpose i.e. not be ‘good enough’. During the construction or delivery of support, these may form the basis of Key Performance Indicators (KPIs). (2) Any other priority is coded P1, P2 (and so on) to reflect lower priorities"
    },
    {
        "question": "What should the Theatre Infrastructure Development Directive (TIDD) address?",
        "answer": "**2C2. Contents of a Theatre Infrastructure Development Directive.** The size and contents of the TIDD will vary over time. As a guide it should address: a. **Infrastructure support to the operation.** The TIDD should explain what infrastructure is required to support the operation, for example, an 'RSOI camp for 5,000', an 'APOD for Sp Ac and 24 Typhoon aircraft', or 'up to 20 company-sized patrol bases'. b. **Planning horizon.** The TIDD should give the planning horizon for infrastructure investments throughout theatre. Ideally, some estimate should be given of the likely tenure of each site and major facility; are they likely to be needed for a short term (less than say six months) or longer term (say up to two years)? c. **Infrastructure programme.** It should state who is responsible for producing the Infrastructure Programme. h. **Health and Safety.** The TIDD may need to give some clarification of the health and safety procedures to be adopted. It may also remind theatre to confirm its infrastructure safety Standard Operating Procedures (SOPs) with PJHQ. Further details are in Part 3. i. **Military works area.** The TIDD should clarify whether a military works area exists and, if so, its boundaries. j. **Lands and environmental issues.** Guidance may be given on the selection of base locations, condition surveys, leasing and environmental assessments, if appropriate.",
    }
]

def select_relevant_examples(query: str, examples: list, max_examples: int = 2):
    """Select the most relevant examples based on token overlap with the query."""
    import re
    
    def token_overlap(text1: str, text2: str) -> int:
        tokens1 = set(re.findall(r'\w+', text1.lower()))
        tokens2 = set(re.findall(r'\w+', text2.lower()))
        return len(tokens1 & tokens2)
    
    # Score examples by token overlap with query
    scored_examples = []
    for example in examples:
        score = token_overlap(query, example["question"])
        scored_examples.append((score, example))
    
    # Sort by score and return top examples
    scored_examples.sort(key=lambda x: x[0], reverse=True)
    return [example for _, example in scored_examples[:max_examples]]
```

I would like to also add another layer of human feedback in the next iteration, a simple thumbs up/thumbs down feedback after each response, so there is metrics to also steer the AI to what the overall userbase would like to see. This obviously could not be the only tool for evaluation, as it could be skewed by bias, but it is a handy extra tool at our hands.