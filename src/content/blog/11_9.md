---
title: 'Looking into other Search Methods'
description: 'Attempting to fix the MOD Nav as the RAG pipeline failed'
pubDate: 'Sep 11 2025'
---

The RAG pipeline failed. The evals I were using failed to pick up on the fundamental flaws of the RAG pipeline with the FAISS indexing. When attempting to ask questions generally on the nature of the document (JTTP 4-05), It was unable to read or understand many parts of the document due to the caching failing, so it could not understand the context it was being provided.

#### Attempts to continue to use the RAG pipeline

First I tried changing the chunk settings for the cache, with many different configurations for both the size and amount of overlap to attempt to get all sections of the system to have useful elements of data. This brought some success, allowing around half of the chunks of data to be useable, going from the original coverage being 18.2% to an ok level of 42.4%. This however is not acceptable for our use case, as we want every detail of the document to be able to be queried and questioned on. This also took all the usefulness of the caching system away, as there had to be an incredibly high number of chunks (7,640), and still had the fundamental issue of not being able to correctly gather all parts of the document:

> Only 3.0% of sections can be reproduced verbatim

> 36.4% of sections completely failed to retrieve

> Zero high similarity matches (â‰¥80%)

This catastrophic failure led to trying a different approach.

#### New system

The next approach I used was a simpler one: Just a strict hierarchal approach, prioritising direct section lookuo first, then via a keyword search, then finally a full text search:

```Python
def _build_context(self, question: str, max_tokens: int) -> LLMContext:
    """Build structured context for the query - OLD HIERARCHICAL APPROACH"""
    
    # 1. Try direct section lookup first (HIGHEST PRIORITY)
    section_number = self._detect_section_request(question)
    if section_number and section_number in self.section_index:
        self._log(f"ðŸŽ¯ Direct section match: {section_number}")
        # IMMEDIATE RETURN - other methods never tried
        return LLMContext(
            query=question,
            relevant_sections=[direct_match],
            search_strategy="direct_section"
        )
    
    # 2. Only if direct search fails, try keyword search
    matches = self._find_keyword_matches(question)
    if matches:
        self._log(f"ðŸ” Found {len(matches)} keyword matches")
        # IMMEDIATE RETURN - full-text search never tried
        return LLMContext(
            query=question,
            relevant_sections=matches[:3],
            search_strategy="keyword_search"
        )
    
    # 3. Only as last resort, try full-text search
    text_matches = self._full_text_search(question)
    if text_matches:
        return LLMContext(
            query=question,
            relevant_sections=text_matches[:3],
            search_strategy="full_text_search"
        )
    
    # 4. Give up if nothing found
    return LLMContext(search_strategy="no_matches")
```

This works to a good standard, as all sections of the document are present and accounted for, and it will answer the majority of questions with very little error. However, it has a number of issues.

1. Information loss - If there is a question surrounding a clause and its content, it may snap to a direct search, which means you are losing valuable insight that could also be elsewhere in the document.

2. No Cross-Validation

#### Further Improvements

After looking at Simon Willison's blog, I decided to use a Reciprocal Rank Fusion approach which is a hybrid combination of all 3 systems:

```Python
def _build_context_with_rrf(self, question: str, max_tokens: int) -> LLMContext:
    """Build context using RRF fusion - NEW PARALLEL APPROACH"""
    
    # 1. Run ALL search methods in parallel
    search_results = {}
    
    # Direct section search (always runs)
    section_number = self._detect_section_request(question)
    if section_number and section_number in self.section_index:
        search_results['direct'] = [create_direct_match(section_number)]
    else:
        search_results['direct'] = []
    
    # Keyword search (always runs)
    search_results['keyword'] = self._find_keyword_matches(question)[:10]
    
    # Full-text search (always runs)  
    search_results['fulltext'] = self._full_text_search(question)[:10]
    
    # 2. Apply RRF to intelligently combine all results
    fused_results = self._reciprocal_rank_fusion(search_results)
    
    # 3. Return best combined results
    return LLMContext(
        query=question,
        relevant_sections=fused_results[:3],
        search_strategy="rrf_fusion"
    )
```
```Python
def _reciprocal_rank_fusion(self, search_results: Dict[str, List[QueryMatch]], k: int = 60):
    """
    Apply Reciprocal Rank Fusion to combine multiple search result lists
    
    RRF Formula: score(d) = Î£(1 / (k + rank_i(d))) for all systems i
    """
    segment_scores = defaultdict(float)
    
    # Strategic weighting based on method strengths
    method_weights = {
        'direct': 3.0,      # Highest - exact section matches are usually perfect
        'keyword': 1.5,     # Medium - good conceptual matches
        'fulltext': 1.0     # Lower - catches edge cases
    }
    
    for method, results in search_results.items():
        weight = method_weights.get(method, 1.0)
        
        for rank, match in enumerate(results, 1):
            segment_id = match.segment.id
            
            # RRF score calculation with method weighting
            rrf_score = weight * (1.0 / (k + rank))
            segment_scores[segment_id] += rrf_score
            
            # Store best match object for each segment
            if segment_id not in segment_objects:
                segment_objects[segment_id] = match
    
    # Return segments sorted by combined RRF score
    return sorted(segment_objects.values(), key=lambda x: x.relevance_score, reverse=True)
```

The new system now does all 3 at once, and weights the best one according to the number of hits it has section wise. This stops information loss as all sections are considered even if there is a direct match in one section. This cross validation is a key feature I want to have in the MOD navigator, as it allows for a greater nuance to be fed into the LLM and provides reliable consistent results.
